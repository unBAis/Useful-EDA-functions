
# gets the unique values from a pandas data frame for each column if its level is smaller than the given number 
def get_unique_values(df, n) : 
    for i in df.columns:
        if df[i].dtype == 'object':
            if len(df[i].unique()) < n:
                print(len(df[i].unique()),"unique values for", i,":",'\n',df[i].unique(), '\n')
            elif len(df[i].unique()) >= n:
                print(len(df[i].unique()),"unique values for", i, "(greater than",n,")",'\n' )

#beside getting the usual summary from describe function , it adds the four more quantiles 5%, 1%,9%, 99% , for a comprehensive view of data at its tails
def get_whole_summary(df):
    quantiles = [.05,.1,.9,.99]
    numeric_cols = []
    quantiles_names = []
    quantiles_num = []
    quantiles_df = []
    summary = df.describe()
    for i in df.columns:
            if df[i].dtype != 'object':
                numeric_cols.append(i)

    for i in numeric_cols:
        for j in quantiles:
            quantiles_names.append(i + "_" + str(j))
            quantiles_num.append(np.quantile(df[i],j)) 

    quantiles_df = pd.DataFrame(quantiles_num, quantiles_names).reset_index()
    quantiles_df.columns = ["variable","value"]
    quantiles_df[[" ","quantile"]] = quantiles_df["variable"].str.rsplit("_",n = 1, expand = True)
    quantiles_df = quantiles_df.drop(["variable"],1)
    quantiles_df = quantiles_df.pivot(index="quantile", columns = " ", values = "value").reset_index()
    sumamrized_df = pd.concat([summary,quantiles_df.set_index("quantile")])
    sumamrized_df.index = ['count',  'mean',   'std',   'min', '25%',   '50%',   '75%',  'max',"5%",  "10%","90%",    "99%"]
    index_reorder = ['count',  'mean',   'std',   'min',"5%",  "10%",'25%', '50%',   '75%', "90%",    "99%", 'max']
    sumamrized_df = sumamrized_df.reindex(index_reorder)
    return sumamrized_df, numeric_cols


#gets the weight of evidence and information value for each column given in the function and return the Dataframe for the same with added information
# the arguments for the function are defined as follows: 
#df = data 
#num_cols = list of numeric/integer columns
#char_cols = list of character columns
#quantile_group = an integer for how many groups are intended for numeric/interger columns distribution
#target = the binary column for which woe needs to be calculated 

def get_woe_df(df, num_cols, char_cols,quantile_group ,target):
    num_lst = []
    char_lst = []
    bin_col_names = []
    
    for i in num_cols:
         bin_col_names.append('bin_'+ i)

    for i,j in zip(num_cols,bin_col_names):
        df[j] = pd.qcut(df[i], q=quantile_group , precision = 0)

        for a in range(df[j].nunique()):
            num_val = list(df[j].unique())[a]
            num_lst.append({
                'Variable': j,
                'Value': num_val,
                'All': df[df[j] == num_val].count()[j],
                'Good': df[(df[j] == num_val) & (df[target] == 0)].count()[j],
                'Bad': df[(df[j] == num_val) & (df[target] == 1)].count()[j]
            })
        num_df = pd.DataFrame(num_lst)

    for k in char_cols:  
        for b in range(df[k].nunique()):
            char_val = list(df[k].unique())[b]
            char_lst.append({
                'Variable': k,
                'Value': char_val,
                'All': df[df[k] == char_val].count()[k],
                'Good': df[(df[k] == char_val) & (df[target] == 0)].count()[k],
                'Bad': df[(df[k] == char_val) & (df[target] == 1)].count()[k]
            })   
        char_df = pd.DataFrame(char_lst)

    woe_df = pd.concat([char_df,num_df])
    woe_df['Distr_Good'] = woe_df['Good'] / woe_df['Good'].sum()
    woe_df['Distr_Bad'] = woe_df['Bad'] / woe_df['Bad'].sum()
    woe_df['WoE'] = np.log(woe_df['Distr_Good'] / woe_df['Distr_Bad'])
    woe_df = woe_df.replace({'WoE': {np.inf: 0, -np.inf: 0}})
    woe_df['IV'] = (woe_df['Distr_Good'] - woe_df['Distr_Bad'])*woe_df['WoE']
    iv = woe_df['IV'].sum()

    woe_df = woe_df.sort_values(by=['Variable','WoE'],ascending = False).reset_index(drop = True)
    IV_df = woe_df.groupby('Variable').agg(Variable_IV = ('IV','sum')).reset_index()
    woe_df = woe_df.merge(IV_df, on = 'Variable', how= 'left')

    return woe_df


#does the chow test where inputs are defined as follows:
#y1 =  pre-event dependent variable data frame
#x1 =  pre-event independent variable/s data frame
#y2 =  post-event dependent variable data frame
#x2 =  post-event independent variable/s data frame

def chow_f(y1, x1, y2, x2):

    def find_rss (y, x):
        A = np.vstack([x.T, np.ones(len(x))]).T
        rss = np.linalg.lstsq(A, y, rcond=None)[1][0]
        length = len(y)
        return (rss, length)


    rss_total, n_total = find_rss(np.append(y1, y2), pd.concat([x1, x2]))
    rss_1, n_1 = find_rss(y1, x1)
    rss_2, n_2 = find_rss(y2, x2)

    df1 = len(x1.columns)+1
    df2 = (n_1 + n_2) - 2*df1
    chow_nom = (rss_total - (rss_1 + rss_2)) / df1
    chow_denom = (rss_1 + rss_2) / df2
    return chow_nom / chow_denom


def chow_p(y1, x1, y2, x2, **kwargs):
    F = chow_f(y1, x1, y2, x2, **kwargs)
    if not F:
        return 1
    df1 = len(x1.columns)+1
    df2 = (len(x1) + len(x2)) - 2*(df1)
    p_val = f.sf(F, df1, df2)
    return p_val
